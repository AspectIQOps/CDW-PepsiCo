# Analytics Platform - Multi-Tool Cost & Usage Analytics

> **Enterprise observability cost management platform**  
> Currently supporting: AppDynamics, ServiceNow  
> Extensible to: Elastic, Datadog, Splunk, New Relic, and more

---

## Overview

The Analytics Platform is a tool-agnostic cost and usage analytics solution designed for enterprise observability tools. It extracts data from multiple sources, transforms it into a unified data model, and provides actionable insights through interactive dashboards.

### Current Features
- ğŸ“Š **AppDynamics License Analytics** - Track licenses, agents, usage patterns
- ğŸ”„ **ServiceNow CMDB Integration** - Application ownership and metadata
- ğŸ’° **Cost Forecasting** - Predict future spend based on historical trends
- ğŸ“ˆ **Grafana Dashboards** - Interactive visualization and reporting
- ğŸ” **Secure Credential Management** - AWS SSM Parameter Store integration
- ğŸ³ **Containerized Deployment** - Docker-based for easy deployment

### Extensibility Framework
Built with a tool-agnostic architecture ready to integrate:
- **Elastic** - Log analytics and search costs
- **Datadog** - Infrastructure monitoring usage
- **Splunk** - Log management and analytics
- **Dynatrace** - Application performance monitoring
- **New Relic** - Full-stack observability

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Sources                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  AppDynamics API  â”‚  ServiceNow API  â”‚  Future Tools    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                  â”‚
           â–¼                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ETL Pipeline (Docker)      â”‚
    â”‚                              â”‚
    â”‚  â€¢ Extract from APIs         â”‚
    â”‚  â€¢ Transform & Enrich        â”‚
    â”‚  â€¢ Load to PostgreSQL        â”‚
    â”‚  â€¢ Validate Data Quality     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PostgreSQL Database        â”‚
    â”‚  (cost_analytics_db)        â”‚
    â”‚                             â”‚
    â”‚  â€¢ AppD Usage/Cost Tables   â”‚
    â”‚  â€¢ ServiceNow CMDB Data     â”‚
    â”‚  â€¢ Cross-Tool Analytics     â”‚
    â”‚  â€¢ Audit & Metadata         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Grafana Dashboards        â”‚
    â”‚                             â”‚
    â”‚  â€¢ License Utilization      â”‚
    â”‚  â€¢ Cost Trends              â”‚
    â”‚  â€¢ Forecasts                â”‚
    â”‚  â€¢ Application Breakdown    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Quick Start

### Prerequisites
- AWS account (EC2, RDS, SSM)
- Docker installed
- PostgreSQL client
- AWS CLI configured

### 1. Clone Repository
```bash
git clone -b deploy-docker https://github.com/AspectIQOps/CDW-PepsiCo.git
cd CDW-PepsiCo
```

### 2. Deploy to AWS
```bash
# Run on EC2 instance
./scripts/setup/ec2_initial_setup.sh
./scripts/setup/setup_ssm_parameters.sh
./scripts/setup/init_database.sh
```

### 3. Start Pipeline
```bash
./scripts/utils/platform_manager.sh start
```

ğŸ“– **See [QUICKSTART.md](QUICKSTART.md) for detailed deployment guide**

---

## Configuration

### Database
- **Name**: `cost_analytics_db`
- **Users**: 
  - `etl_analytics` - ETL pipeline operations
  - `grafana_ro` - Dashboard read-only access

### SSM Parameter Structure
```
/pepsico/
â”œâ”€â”€ DB_HOST, DB_NAME, DB_USER, DB_PASSWORD
â”œâ”€â”€ appdynamics/
â”‚   â”œâ”€â”€ CONTROLLER
â”‚   â”œâ”€â”€ ACCOUNT
â”‚   â”œâ”€â”€ CLIENT_NAME
â”‚   â””â”€â”€ CLIENT_SECRET
â”œâ”€â”€ servicenow/
â”‚   â”œâ”€â”€ INSTANCE
â”‚   â”œâ”€â”€ USER
â”‚   â””â”€â”€ PASS
â””â”€â”€ (future tools)/
```

### Environment Variables
```bash
DB_NAME=cost_analytics_db
DB_USER=etl_analytics
AWS_REGION=us-east-2
SSM_BASE_PATH=/pepsico
```

---

## Usage

### Platform Management

```bash
# Start pipeline
./scripts/utils/platform_manager.sh start

# Check status
./scripts/utils/platform_manager.sh status

# View logs
./scripts/utils/platform_manager.sh logs

# Run health check
./scripts/utils/platform_manager.sh health

# Validate data
./scripts/utils/platform_manager.sh validate

# Stop pipeline
./scripts/utils/platform_manager.sh stop
```

### Database Access

```bash
# Connect via platform manager
./scripts/utils/platform_manager.sh db

# Or manually
PGPASSWORD=$(aws ssm get-parameter --name /pepsico/DB_PASSWORD --with-decryption --query 'Parameter.Value' --output text --region us-east-2) \
psql -h YOUR_RDS_ENDPOINT -U etl_analytics -d cost_analytics_db
```

---

## Project Structure

```
CDW-PepsiCo/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup/              # Initial deployment scripts
â”‚   â”‚   â”œâ”€â”€ ec2_initial_setup.sh
â”‚   â”‚   â”œâ”€â”€ setup_ssm_parameters.sh
â”‚   â”‚   â””â”€â”€ init_database.sh
â”‚   â”‚
â”‚   â”œâ”€â”€ etl/                # ETL pipeline code
â”‚   â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â”‚   â”œâ”€â”€ appd_etl.py
â”‚   â”‚   â””â”€â”€ servicenow_etl.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/              # Operational utilities
â”‚       â”œâ”€â”€ platform_manager.sh     # Main management tool
â”‚       â”œâ”€â”€ health_check.sh
â”‚       â”œâ”€â”€ verify_setup.sh
â”‚       â””â”€â”€ validate_pipeline.py
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init/               # Database schema
â”‚       â”œâ”€â”€ 01_init_users_and_schema.sql
â”‚       â”œâ”€â”€ 02_create_appd_tables.sql
â”‚       â””â”€â”€ 03_create_servicenow_tables.sql
â”‚
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ QUICKSTART.md       # â­ Start here
â”‚   â”œâ”€â”€ AWS_EC2_SETUP.md
â”‚   â”œâ”€â”€ AWS_RDS_SETUP.md
â”‚   â””â”€â”€ RENAME_SUMMARY.md
â”‚
â”œâ”€â”€ docker-compose.ec2.yaml # Container orchestration
â”œâ”€â”€ Dockerfile              # Container build
â””â”€â”€ .env.example            # Environment template
```

---

## Key Features

### ğŸ”„ Multi-Tool Support
- Tool-agnostic data model
- Separate ETL pipelines per tool
- Unified analytics across tools
- Easy to add new integrations

### ğŸ“Š AppDynamics Integration
- License usage tracking
- Agent inventory
- Cost allocation by application
- Forecasting and trends

### ğŸ—‚ï¸ ServiceNow CMDB
- Application metadata
- Ownership information
- Sector/capability mapping
- Cross-reference with usage data

### ğŸ” Security
- No hardcoded credentials
- AWS SSM Parameter Store
- IAM role-based access
- Read-only dashboard user

### ğŸ“ˆ Analytics
- Historical trend analysis
- Cost forecasting
- Usage pattern detection
- Anomaly identification

---

## Data Model

### Core Tables

#### Tool Configurations
```sql
tool_configurations
â”œâ”€â”€ tool_name (appdynamics, servicenow, elastic, etc.)
â”œâ”€â”€ is_active
â”œâ”€â”€ last_successful_run
â””â”€â”€ configuration (JSONB)
```

#### Audit Trail
```sql
audit_etl_runs
â”œâ”€â”€ run_id (UUID)
â”œâ”€â”€ tool_name
â”œâ”€â”€ pipeline_stage
â”œâ”€â”€ start_time / end_time
â”œâ”€â”€ status
â””â”€â”€ metadata (JSONB)
```

#### AppDynamics Tables
- `appd_applications` - Application catalog
- `appd_licenses` - License inventory
- `appd_agents` - Agent inventory
- `appd_usage_daily` - Daily usage metrics
- `appd_cost_forecasts` - Projected costs

#### ServiceNow Tables
- `servicenow_cmdb` - CMDB records
- Ownership and metadata

---

## Extending the Platform

### Adding a New Tool

1. **Create SSM Parameters**
```bash
aws ssm put-parameter --name /pepsico/newtool/API_KEY --value 'your-key' --type SecureString
```

2. **Add Tool Configuration**
```sql
INSERT INTO tool_configurations (tool_name, is_active)
VALUES ('newtool', TRUE);
```

3. **Create ETL Script**
```python
# scripts/etl/newtool_etl.py
# Follow pattern from appd_etl.py
```

4. **Create Tables**
```sql
-- sql/init/XX_create_newtool_tables.sql
CREATE TABLE newtool_data (...);
```

5. **Update Docker Compose**
```yaml
services:
  etl-newtool:
    environment:
      - SSM_NEWTOOL_PREFIX=/pepsico/newtool
```

ğŸ“– **See [RENAME_SUMMARY.md](docs/RENAME_SUMMARY.md) for detailed extensibility guide**

---

## Monitoring & Operations

### Health Checks
```bash
# Comprehensive health check
./scripts/utils/platform_manager.sh health

# Check individual components
- Docker status
- AWS IAM role
- Database connectivity
- Required tables
- Disk space
```

### Data Validation
```bash
# Run data quality checks
./scripts/utils/platform_manager.sh validate

# Validates:
- Record counts
- Data freshness
- Completeness
- Referential integrity
```

### Logs
```bash
# Live logs
./scripts/utils/platform_manager.sh logs

# Logs location
./logs/etl_YYYYMMDD_HHMMSS.log
```

---

## Cost Optimization

### Daily Runtime (8-10 hours)
- EC2 t3.medium: ~$0.35/day
- RDS db.t3.medium: ~$0.60/day
- **Total: ~$1.00/day** (~$30/month)

### Cost-Saving Tips
```bash
# Stop when not in use
./scripts/utils/platform_manager.sh stop

# Stop RDS instance
aws rds stop-db-instance --db-instance-identifier your-db --region us-east-2

# Use reserved instances for production
# Schedule ETL runs for off-peak hours
```

---

## Troubleshooting

### Common Issues

**Database Connection Failed**
```bash
# Check security group allows EC2 â†’ RDS
# Verify credentials in SSM
./scripts/utils/platform_manager.sh health
```

**SSM Parameters Not Found**
```bash
# List parameters
./scripts/utils/platform_manager.sh ssm

# Verify IAM role has SSM read permissions
aws sts get-caller-identity
```

**Docker Build Fails**
```bash
# Rebuild without cache
docker compose -f docker-compose.ec2.yaml build --no-cache

# Check logs
docker compose -f docker-compose.ec2.yaml logs
```

---

## Development

### Local Development
```bash
# Use mock data
export USE_MOCK_DATA=true

# Run locally
python3 scripts/etl/run_pipeline.py

# Run tests
pytest tests/
```

### Adding Features
1. Create feature branch
2. Update ETL scripts
3. Add SQL migrations
4. Update documentation
5. Test end-to-end
6. Submit PR

---

## Documentation

- ğŸ“˜ [Quick Start Guide](QUICKSTART.md) - Get up and running
- ğŸ“— [EC2 Setup Guide](docs/AWS_EC2_SETUP.md) - Detailed EC2 configuration
- ğŸ“™ [RDS Setup Guide](docs/AWS_RDS_SETUP.md) - Detailed RDS configuration
- ğŸ“• [Rename Summary](docs/RENAME_SUMMARY.md) - Architecture and extensibility
- ğŸ““ [Daily Checklist](docs/DAILY_CHECKLIST.md) - Daily operations

---

## Contributing

This is currently a private repository for PepsiCo's internal use. If you have access and want to contribute:

1. Follow the development workflow above
2. Maintain consistent naming conventions
3. Update documentation for any changes
4. Test thoroughly before committing

---

## License

Proprietary - PepsiCo Internal Use Only

---

## Support

For issues or questions:
1. Check documentation in `docs/`
2. Review troubleshooting section above
3. Run health checks: `./scripts/utils/platform_manager.sh health`
4. Contact: AspectIQ Operations Team

---

## Technology Stack

- **Language**: Python 3.11+
- **Database**: PostgreSQL 16.3
- **Container**: Docker & Docker Compose
- **Cloud**: AWS (EC2, RDS, SSM)
- **Orchestration**: Custom ETL pipeline
- **Visualization**: Grafana
- **APIs**: AppDynamics REST API, ServiceNow REST API

---

## Roadmap

### Current Phase (Q4 2024)
- âœ… AppDynamics integration
- âœ… ServiceNow CMDB integration
- âœ… Cost forecasting
- âœ… Grafana dashboards

### Next Phase (Q1 2025)
- â³ Elastic integration
- â³ Enhanced forecasting models
- â³ Automated alerting
- â³ API endpoints for external access

### Future Phases
- ğŸ”® Datadog integration
- ğŸ”® Splunk integration
- ğŸ”® Machine learning for anomaly detection
- ğŸ”® Self-service dashboard builder

---